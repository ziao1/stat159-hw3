computeMLE <- function(x){
# treat x as a vector
# let x[1] be n_1, x[2] be n_2, x[3] be n_3
n_1 = x[1]
n_2 = x[2]
n_3 = x[3]
(n_2 + 2*n_3) / (2*n_1 + 2*n_2 + 2*n_3)
}
n <- 10 + 68 + 112
theta_hat <- computeMLE(c(10, 68, 112))
prob = c( (1 - theta_hat)^2, 2*theta_hat*(1 - theta_hat), theta_hat^2)
B <- 1000 # or any large number
boot_samps <- rmultinom(B, n, prob)
boot_mle <- apply(boot_samps, 2, computeMLE)
sd(boot_mle)
deltas <- quantile(theta_hat, probs = c(0.995, 0.005))
confint <- 2*theta_hat - deltas
deltas <- quantile(boot_mle, probs = c(0.995, 0.005))
confint <- 2*boot_mle - deltas
confint <- 2*theta_hat - deltas
computeMLE = function(x){
# treat x as a vector
# let x[1] be n_1, x[2] be n_2, x[3] be n_3
n_1 = x[1]
n_2 = x[2]
n_3 = x[3]
(n_2 + 2*n_3) / (2*n_1 + 2*n_2 + 2*n_3)
}
n = 10 + 68 + 112
theta_hat = computeMLE(c(10, 68, 112))
prob = c( (1 - theta_hat)^2, 2*theta_hat*(1 - theta_hat), theta_hat^2)
B = 5000
boot_samps = rmultinom(B, n, prob)
boot_mle = apply(boot_samps, 2, computeMLE)
sd(boot_mle)
deltas = quantile(boot_mle, probs = c(0.995, 0.005))
confint = 2*theta_hat - deltas
install.packages("foreign")
write.dta(wsbhaiti, “wsbhaiti.dta”)
wsbhaiti = read.dta("wsbhaiti.dta")
controls = c(41.0, 38.4, 25.9, 21.9, 13.1, 27.3, -16.9, 17.4, 15.4, 27.4, 22.4, 17.7, 29.4, 21.4, 26.0, 26.6, 24.9, 18.3, 28.5, 21.8, 19.2, 26.0, 22.7)
ozone = c(6.1, 20.4, 14.3, 15.5, 6.8, 28.2, -12.9, 14.0, 12.1, 15.7, -15.9, 54.6, 44.1, -9.0, 10.1, 7.3, -9.9, 17.9, 6.6, 39.9, -14.7, -9.0)
t.test(controls,ozone,alternative = "two.sided",var.equal = F)
t.test(controls,ozone,alternative = "less",var.equal = F)
t.test(controls,ozone,alternative = "greater",var.equal = F)
X_data = c(3.03, 5.53, 5.60, 9.30, 9.92, 12.51, 12.95, 15.21, 16.04, 16.84)
Y_data = c(3.19, 4.26, 4.47, 4.53, 4.67, 4.69, 12.78, 6.79, 9.37, 12.75)
n = length(X_data)
m = length(Y_data)
boot_samps <- replicate(B, c( sample(X_data, size = n, replace = TRUE),
sample(Y_data, size = m, replace=TRUE)))
boot_samps <- replicate(50, c( sample(X_data, size = n, replace = TRUE),
sample(Y_data, size = m, replace=TRUE)))
boot_estims <- apply(boot_samps, 2,
function(x){ wilcox.test(x ~ grouping_var)$statistic })
View(boot_samps)
grouping_var = c(replicate(10,X), replicate(10,Y))
grouping_var = c(X,X,X)
grouping_var = c(replicate(10,"X"), replicate(10, "Y"))
boot_estims = apply(boot_samps, 2,
function(x){ wilcox.test(x ~ grouping_var)$statistic })
warnings()
boot_estims
deltas = quantile(boot_estims, probs = c(0.95, 0.05))
confint = 2*75 - deltas
test = c(676, 206, 230, 256, 280, 433, 337, 466, 497, 512, 794, 428, 452, 512)
control = c(88, 570, 605, 617, 653, 2913, 924, 286, 1098, 982, 2346, 321, 615, 519)
a = length(test)
b = length(control)
difference = test - control
plot(difference, control)
plot(difference, control, type="l")
lines(difference, control)
difference = test - control
plot(difference, control)
d = test - control
plot(d, control)
d_bar = mean(d)
d_sd = sd(d)
boot_samps = replicate(50, sample(d, size = length(d), replace = TRUE)))
boot_samps = replicate(50, sample(d, size = length(d), replace = TRUE))
boot_samps = replicate(50, c( sample(X_data, size = n, replace = TRUE),
sample(Y_data, size = m, replace=TRUE)))
boot_samps2 = replicate(50, sample(d, size = length(d), replace = TRUE))
X_data = c(3.03, 5.53, 5.60, 9.30, 9.92, 12.51, 12.95, 15.21, 16.04, 16.84)
Y_data = c(3.19, 4.26, 4.47, 4.53, 4.67, 4.69, 12.78, 6.79, 9.37, 12.75)
n = length(X_data)
m = length(Y_data)
boot_samps = replicate(50, c( sample(X_data, size = n, replace = TRUE),
sample(Y_data, size = m, replace=TRUE)))
grouping_var = c(replicate(10,"X"), replicate(10, "Y"))
boot_estims = apply(boot_samps, 2,
function(x){ wilcox.test(x ~ grouping_var)$statistic })
deltas = quantile(boot_estims, probs = c(0.95, 0.05))
confint = 2*75 - deltas
### Q39
# part a
test = c(676, 206, 230, 256, 280, 433, 337, 466, 497, 512, 794, 428, 452, 512)
control = c(88, 570, 605, 617, 653, 2913, 924, 286, 1098, 982, 2346, 321, 615, 519)
d = test - control
plot(d, control)
# part b
d_bar = mean(d)
d_sd = sd(d)
boot_samps2 = replicate(50, sample(d, size = length(d), replace = TRUE))
View(boot_samps2)
d_median = median(d)
boot_mean = apply(boot_samps2, 2, mean)
boot_median = apply(boot_samps2, 2, median)
deltas1 = quantile(boot_mean, probs = c(0.95, 0.05))
confint = 2*(d_bar) - deltas1
confint = 2*75 - deltas
confint1 = 2*(d_bar) - deltas1
deltas2 = quantile(boot_median, probs = c(0.95, 0.05))
confint2 = 2*(d_median) - deltas2
field_present = c(22.8, 10.2, 20.8, 27.0, 19.2, 9.0, 14.2, 19.8, 14.5, 14.8)
filed_absent = c(23.5, 31.0, 19.5, 26.2, 26.5, 25.2, 24.5, 23.8, 27.8, 22.0)
length(field_present)
var(field_present)
var(field_absent)
field_absent = c(23.5, 31.0, 19.5, 26.2, 26.5, 25.2, 24.5, 23.8, 27.8, 22.0)
var(field_absent)
t.test(field_present, field_absent, alternative="two.sided", var.equal=F)
boot_estims
boot_estims/100
confint/100
plot(d, control)
d_bar
d_sd
d_median
confint1
confint2
library("spls")
library(rpart)
library(rpart.plot)
library(ipred)
library(randomForest)
setwd("~/Dropbox/School/Grad/ST154-2016/HW/HW5/")
source("../HW4/Codes/ClassificationMetrics.R")
getwd()
data(prostate)
library("spls")
library(rpart)
library(rpart.plot)
library(ipred)
library(randomForest)
#setwd("~/Dropbox/School/Grad/ST154-2016/HW/HW5/")
#source("../HW4/Codes/ClassificationMetrics.R")
# 1. Load Data
data(prostate)
X = prostate$x
df = data.frame(prostate$x)
df$target = prostate$y
y = as.factor(prostate$y)
n = nrow(df)
n_fold = 5
# 2. Function to do Cross-Validation
GetFoldID = function(n, n_fold, seed = 2) {
fold_id = rep(1:n_fold, 1 + n / n_fold)
set.seed(seed)
fold_id = sample(fold_id, size = n)
fold_id = fold_id[1:n]
return(fold_id)
}
fold_id = GetFoldID(n, n_fold)
# 3. Running rpart
if (TRUE) {
yhat_tree = numeric(n)
for (i in 1:n_fold) {
print(i)
fit = rpart(target ~ ., data = df[fold_id != i, ], method = "class",
control = rpart.control(maxdepth = 6, minsplit = 3, cp = .25))
yhat_tree[fold_id == i] = predict(fit,
newdata = df[fold_id == i, ],
type = "prob")[,2]
}
all_scores(df$target, yhat_tree)
fit <- rpart(target ~ ., data = df,
method="class",
maxdepth = 6, minsplit = 3, cp = .25)
plot(fit)
text(fit, use.n=TRUE, all=TRUE, cex=.8)
post(fit, file = "./Figures/tree.ps",
title = "Classification Tree for Prostate")
rpart.plot(fit)
}
# 4.Bagging
library(glmnet)
library(MASS)
#setwd("~/Dropbox/School/Grad/ST154-2016/HW/HW4/Codes/")
#source("./ClassificationMetrics.R")
### 1. Load Data
df = read.table(
"http://statweb.stanford.edu/~tibs/ElemStatLearn/datasets/SAheart.data",
sep=",", head = T, row.names = 1)
y = df$chd
df$chd = NULL
# Convert String to Binary Variable
df$famhist = df$famhist == "Present"
X = as.matrix(df)
### 2.
GetLDAProb = function(Xtrain, ytrain, Xvalid) {
model = lda(x = Xtrain, grouping = ytrain)
predict(model, newdata = Xvalid)$posterior[,2]
}
GetQDAProb = function(Xtrain, ytrain, Xvalid) {
model = qda(x = Xtrain, grouping = ytrain)
predict(model, newdata = Xvalid)$posterior[,2]
}
GetLogisticL1Prob = function(Xtrain, ytrain, Xvalid, lambda) {
model = glmnet(x = Xtrain, y = ytrain, family = "binomial", lambda = lambda)
predict(model, Xvalid)
}
GetLogisticL1ProbFunc = function(lambda) {
return(function(Xtrain, ytrain, Xvalid) GetLogisticL1Prob(Xtrain, ytrain, Xvalid, lambda))
}
GetFoldID = function(n, n_fold) {
fold_id = rep(1:n_fold, 1 + n / n_fold)
set.seed(2)
fold_id = sample(fold_id, size = n)
fold_id = fold_id[1:n]
return(fold_id)
}
GetCVPrediction = function(model, X, y, n_fold) {
n = length(y)
fold_id = GetFoldID(n, n_fold)
prob = numeric(n)
for (fold in 1:n_fold) {
prob[fold_id == fold] = model(X[fold_id != fold, ], y[fold_id != fold],
X[fold_id == fold, ])
}
return(prob)
}
lda_prob = GetCVPrediction(GetLDAProb, X, y, 5)
all_scores(y, lda_prob)
getwd()
setwd("/Users/Simon/Desktop/stat159-hw3")
Advertising = read.csv("~/Users/Simon/Desktop/stat159-hw3/data/Advertising.csv")
Advertising = read.csv("/Users/Simon/Desktop/stat159-hw3/data/Advertising.csv")
sink(file ="./data/eda-output.txt")
cat("TV Summary")
cat("\n")
summary(Advertising$TV)
cat("\n")
cat("Radio Summary")
cat("\n")
summary(Advertising$Radio)
cat("\n")
cat("Newspaper Summary")
cat("\n")
summary(Advertising$Newspaper)
cat("\n")
cat("Sales Summary")
cat("\n")
summary(Advertising$Sales)
cat("\n")
cat("Correlation Matrix")
cat("\n")
cor(Advertising)
sink()
correlation_matrix = cor(Advertising)
save(correlation_matrix,
file = "./data/correlation-matrix.RData")
jpeg("./images/histogram-tv.png")
hist(Advertising$TV, main = "TV Advertising Histogram", xlab = "Money Spent on TV Advertising (in thousands)", ylab = "Number of Markets")
dev.off()
jpeg("./images/histogram-radio.png")
hist(Advertising$Radio, main = "Radio Advertising Histogram", xlab = "Money Spent on Radio Advertising (in thousands)", ylab = "Number of Markets")
dev.off()
jpeg("./images/histogram-newspaper.png")
hist(Advertising$Newspaper, main = "Newspaper Advertising Histogram", xlab = "Money Spent on Newspaper Advertising (in thousands)", ylab = "Number of Markets")
dev.off()
jpeg("./images/histogram-sales.png")
hist(Advertising$Sales, main = "Sales Histogram", xlab = "Money Spent on TV Advertising (in thousands)", ylab = "Number of Markets")
dev.off()
pairs(Advertising)
jpeg("./images/scatterplot-matrix.png")
pairs(Advertising)
dev.off()
library("xtable")
install.packages("xtable")
library("xtable")
library("xtable")
regression_object = lm(Sales~TV, data = Advertising)
reg_summary = summary(regression_object)
save(regression_object,
reg_summary,
file = "./data/regression.RData")
jpeg("./images/residual-plot.png")
plot(regression_object, which = c(1))
dev.off()
jpeg("./images/scale-location-plot.png")
plot(regression_object, which = c(2))
dev.off()
jpeg("./images/normal-qq-plot.png")
plot(regression_object, which = c(3))
dev.off()
library(testthat)
sink(file ="./session-info-script.txt")
sessionInfo()
sink()
regression_object
jpeg('./images/scatterplot-tv-sales.png')
plot(sales ~ tv)
dev.off()
jpeg('./images/scatterplot-tv-sales.png')
plot(Advertising$sales ~ Advertising$tv)
dev.off()
jpeg('./images/scatterplot-tv-sales.png')
plot(Advertising$Sales ~ Advertising$TV)
dev.off()
# First create for each three predictors
# Then create the overall regression
# scatterplot-tv-sales
jpeg('./images/scatterplot-tv-sales.png')
plot(Advertising$Sales ~ Advertising$TV)
dev.off()
# regression of sales - tv
reg_tv = lm(Advertising$Sales ~ Advertising$TV)
# scatterplot-radio-sales
jpeg('./images/scatterplot-radio-sales.png')
plot(Advertising$Sales ~ Advertising$Radio)
dev.off()
# regression of sales - radio
reg_radio = lm(Advertising$Sales ~ Advertising$Radio)
# scatterplot-newspaper-sales
jpeg('./images/scatterplot-newspaper-sales.png')
plot(sales ~ news, cex=1, pch=19, col='orange')
dev.off()
# regression of sales - newspaper
reg_news = lm(Advertising$Sales ~ Advertising$Newspaper)
# scatterplot-newspaper-sales
jpeg('./images/scatterplot-newspaper-sales.png')
plot(Advertising$Sales ~ Advertising$Newspaper)
dev.off()
# regression of sales - newspaper
reg_news = lm(Advertising$Sales ~ Advertising$Newspaper)
reg_all = lm(Advertising$Sales ~ Advertising$TV+Advertising$Radio+Advertising$Newspaper)
# Residual plot
residual = summary(reg_all)$residuals
jpeg('./images/residual-plot.png')
plot(reg_all, which=1)
dev.off()
# Scale-location-plot
jpeg('./images/scale-location-plot.png')
plot(reg_all, which=3)
dev.off()
# Normal Q-Q plot
jpeg('./images/normal-qq-plot.png')
plot(reg_all, which=2)
dev.off()
## Save regression.RData
save(reg_all, reg_tv, reg_radio, reg_news, file='data/regression.RData')
library(testthat)
test_file("code/tests/test-regression.R")
source("../functions/regression-functions.R")
source("./functions/regression-functions.R")
source("functions/regression-functions.R")
source("./User/Simon/Desktop/stat159-hw3/code/functions/regression-functions.R")
source("User/Simon/Desktop/stat159-hw3/code/functions/regression-functions.R")
getwd()
source("code/functions/regression-functions.R")
context("Testing quality statistic calculations")
# Multiple Regression
reg <- lm(mpg ~ disp + hp, data = mtcars)
regsum <- summary(reg)
# Test cases
test_that("rss is correct", {
x <- residual_sum_squares(reg)
expect_equal(x, sum(reg$residuals^2))
expect_length(x, 1)
expect_type(x, 'double')
})
library(testthat)
test_that("rss is correct", {
x <- residual_sum_squares(reg)
expect_equal(x, sum(reg$residuals^2))
expect_length(x, 1)
expect_type(x, 'double')
})
test_that("rss is correct", {
x <- residual_sum_squares(reg)
expect_equal(x, sum(reg$residuals^2))
#  expect_length(x, 1)
expect_type(x, 'double')
})
load("regression.RData")
getwd()
setwd("/Users/Simon/Desktop/stat159-hw3/data)
load("regression.RData")
setwd("/Users/Simon/Desktop/stat159-hw3/data")
```{r,echo=FALSE}
load("regression.RData")
Table 1: Information about regression coefficients
```{r,echo=FALSE}
load("regression.RData")
coeff
load("regression.RData")
load("regression.RData")
reg_all$coeff
```{r,echo=FALSE}
load("regression.RData")
reg_all$coeff
```
reg_news$coeff
Advertising = read.csv("/Users/Simon/Desktop/stat159-hw3/data/Advertising.csv")
correlation_matrix = cor(Advertising)
save(correlation_matrix,
file = "./data/correlation-matrix.RData")
load("correlation_matrix.RData")
load("correlation-matrix.RData")
correlation_matrix
load("regression.RData")
reg_all$coeff
reg_news$coeff
reg_radio$coeff
reg_tv$coeff
```
getwd()
setwd("/Users/Simon/Desktop/stat159-hw3")
setwd("/Users/Simon/Desktop/fall2016/stat159/stat159_hw2")
setwd("/Users/Simon/Desktop/fall2016/stat159/stat159_hw2/data")
load("regression.RData")
setwd("/Users/Simon/Desktop/stat159-hw3")
setwd("/Users/Simon/Desktop/stat159-hw3/data")
